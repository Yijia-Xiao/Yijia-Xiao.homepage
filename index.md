My name is **Yijia Xiao**.

I am currently a senior undergraduate from Department of Computer Science and Technology, Tsinghua University. I have been working as a research assistant at [Tsinghua KEG](http://keg.cs.tsinghua.edu.cn/) since 2019, advised by [Prof. Jie Tang](https://keg.cs.tsinghua.edu.cn/jietang/).


## Research Interests
My research interests include computational biology, natural language processing. I am particularly interested in protein folding and virtual screening in computational biology, as well as their applications in drug discovery.


## Projects

<span style="color:red">News</span>: The 6-billion-parameter model is being pretrained on 50 million MSA samples. The process takes about 3-4 month. More details and updates will be availabel at this [page](https://Yijia-Xiao.github.io/publications/protein-msa.html).

### Protein-LM
Pre-trained super-scale protein language models with Megatron-LM. The largest one (ProteinLM-3B) contains 3 billion parameters, the second largest protein language model in the world. Protein-LM achieved excellent results on benchmarks provided by [TAPE](https://arxiv.org/abs/1906.08230). ProteinLM-3B far exceeds the baseline of protein folding contact map prediction.

Besides, the work is named *Wen Su* in Chinese, and it is one key partition of WuDao AI [Wikipedia](https://en.wikipedia.org/wiki/Wu_Dao). The protein model has beed used by more than 20 prestigious institutions like [Harvard](https://www.harvard.edu/) and [University of Toronto](https://www.utoronto.ca/). If you are interested in the model file, please apply from [WuDao AI](https://resource.wudaoai.cn/).

<!-- [Harvard](https://www.harvard.edu/) and [University of Toronto](https://www.utoronto.ca/) -->

Implementation and model files are available [here](https://github.com/THUDM/ProteinLM).

### Protein-MSA
I am fortunate to work as a research assistant at [Toyota Technological Institute at Chicago](https://www.ttic.edu/) in 2021, advised by [Prof. Jinbo Xu](https://home.ttic.edu/~jinbo/).

Pre-trained a 1-billion-parameter model (the world's largest) with proposed *fragment training strategy* on 1.5 million MSAs. Exceeded the performance of Facebook's work with only 10% of training data (much higher data utilization)
<!-- Unsupervised Contact Prediction with Protein SA. -->

Implementation and model files are available [here](https://github.com/Yijia-Xiao/Protein-MSA).

Detailed information is available [here](https://Yijia-Xiao.github.io/publications/protein-msa.html).

## Publications

\* for corresponding author

- **[Modeling Protein Using Large-scale Pretrain Language Model](https://Yijia-Xiao.github.io/files/Modeling_Protein_Using_Large-scale_Pretrain_Language_Model.pdf)** <br/> ***Yijia Xiao***, *Jiezhong Qiu*, *Ziang Li*, *Chang-Yu Hsieh*, *\*Jie Tang* <br/> KDD 2021 Pretrain Workshop (The International Workshop on Pretraining: Algorithms, Architectures, and Applications)

- **[SPLDExtraTrees: Robust machine learning approach for predicting kinase inhibitor resistance](https://arxiv.org/abs/2111.08008)** <br/> *Ziyi Yang*, *Zhaofeng Ye*, ***Yijia Xiao***, *\*Changyu Hsieh* <br/> Briefings in Bioinformatics (*under review*)

- **[A Deep Learning Inference Scheme Based on Pipelined Matrix Multiplication Acceleration Design and Non-uniform Quantization](https://Yijia-Xiao.github.io/files/A_Deep_Learning_Inference_Scheme.pdf)** <br/> *Yuyang Zhang*, *\*Dik Hin Leung*, *Min Guo*, ***Yijia Xiao***, *Haoyue Liu*, *Yunfei Li*, *Jiyuan Zhang*, *Guan Wang*, *Zhen Chen* <br/> IEEE CCIS 2021 (IEEE International Conference on Cloud Computing and Intelligence Systems)

## Find me

- [Email](mailto:mr.yijia.xiao@gmail.com)

- [Google Scholar](https://scholar.google.com/citations?user=I8Y114YAAAAJ&hl=en)

- [Github](https://github.com/Yijia-Xiao)

- [Linkedin](https://www.linkedin.com/in/yijia-xiao/)


<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=kI-i930V6akQPyUWlqEbKYEq76tgAxp4CdYHMgd9f4s&cl=ffffff&w=a"></script>
